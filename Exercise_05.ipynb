{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6466584d",
   "metadata": {},
   "source": [
    "Matrix Multiplication is defined as follows\n",
    "\n",
    "$$R_{ij} = \\sum_k A_{ik}B_{kj}$$\n",
    "\n",
    "\n",
    "## Step 1\n",
    "\n",
    "Derive a formula for how many basic operations are required for a matrix multiplication\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81409d8e",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "* there are $i\\cdot j$ entries in the resulting matrix\n",
    "* for each entry there are k terms we need to add to the result of this entry (1 addition) and to compute what to add we need to compute $A_{ik}B_{kj}$ (1 multiplication)\n",
    "* $2k$ basic operations for each entry and $i\\cdot j$ entries\n",
    "* $2ijk$ operations per matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192bbede",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "Implement an non-fancy algorithm (just use python loops) that multiplies to matrices and returns\n",
    "  * The resulting matrix R\n",
    "  * A count of the number of additions and multiplications (i.e. floating point operations) required for the computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9c34f2",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "* Check that your code works by comparing the result with `np.matmul` using the function `np.allclose` and using random matrices (from a standard normal)\n",
    "* Check your formula for the full number of floating point operations (FLOPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0217c02",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "\n",
    "Consider this decreasing set of matrices\n",
    "* A = Matrix with shape (100,200)\n",
    "* B = Matrix with shape (50,100)\n",
    "* C = Matrix with shape (2,50)\n",
    "\n",
    "We are interested in computing $M = ABC$ and we could do it 2 ways\n",
    "\n",
    "* The \"forward\" way $C(BA)$\n",
    "* The \"backward\" way $(CB)A$\n",
    "\n",
    "* Use your matrix multiply to compute the computational cost of these two options\n",
    "* cross-check with your formula that this matches expectations\n",
    "* Which one is more advantagous for a linear map from 200 â†’ 2 dimensions? What's the ratio of the reuqired FLOPs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9d5282",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "\n",
    "Repeat the same Exercise for an increasing set of matrices\n",
    "\n",
    "\n",
    "* A = Matrix with shape (300,2)\n",
    "* B = Matrix with shape (500,300)\n",
    "* C = Matrix with shape (1000,500)\n",
    "\n",
    "Which direction is more advantages now?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04402f5",
   "metadata": {},
   "source": [
    "## Step 6\n",
    "\n",
    "Take the function $$\\mathbb{R}^3 \\to \\mathbb{R}^2$$\n",
    "\n",
    "* Install the library JAX `pip install jax jaxlib`\n",
    "\n",
    "and consider the following function\n",
    "\n",
    "```python\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def func(x):\n",
    "    x1,x2,x3 = x\n",
    "    z1 = 2*x1*x2\n",
    "    z2 = x3**2\n",
    "    return jnp.array([z1,z2])\n",
    "```\n",
    "\n",
    "* What is the Jacobian of this function $\\frac{\\partial z_i}{\\partial x_i}$?\n",
    "* What does the Jacobian look like at the input `X = [2.,3.,2.]` ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d94f661",
   "metadata": {},
   "source": [
    "## Step 7\n",
    "\n",
    "`jax` allows us to extract the Jacobian via Jacobian Vector Products of Vector Jacobian products\n",
    "\n",
    "To compute $Jv$ at a point $x$ we can use `value, jac_column = jax.jvp(function, (x,), (v,))` \n",
    "\n",
    "This will give us `value` corresponding to $f(x)$ and `jac_column` corresponding to $Jv$\n",
    "\n",
    "* Use this API to extract the 3 columns of the Jacobian via 3 special choices of $v$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2722876",
   "metadata": {},
   "source": [
    "## Step 8\n",
    "\n",
    "`jax` also allows us to to VJPs. As we know these are the ones that are most important for Deep Learning\n",
    "\n",
    "Here the API is slighly different. We can call `value, vjp_func = jax.vjp(function,x)`\n",
    "The variable `value` will correspond as usual to $f(x)$, while `jvp_func` is a function that we can call\n",
    "with `vjp_func(v)` to compute $v^T J$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107e3aaa",
   "metadata": {},
   "source": [
    "Congratulations - you've now used your first automatic differentiation system!\n",
    "\n",
    "To celebrate let's calculate the gradient of some fun functions\n",
    "\n",
    "In JAX we can do this like this\n",
    "\n",
    "```python\n",
    "def myfunction(x):\n",
    "    return x**2\n",
    "\n",
    "\n",
    "value_and_gradient_func = jax.vmap(jax.value_and_grad(myfunction))\n",
    "```\n",
    "\n",
    "`value_and_gradient_func` now is a function in which we an pass an array of x values and it will return an array\n",
    "of y values and the gradient value $\\frac{\\partial f}{\\partial x}$ at that point.\n",
    "\n",
    "```python\n",
    "values, gradients = value_and_gradient_func(xarray)\n",
    "```\n",
    "\n",
    "Use this to compute the function value and gradients for `x^2`, `\\sin(x^2)\\cos(x)`, or whatever function you can thing of on the interval (-5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0647d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
